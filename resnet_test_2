import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress warnings and informational logs
import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
from sklearn.neighbors import NeighborhoodComponentsAnalysis
from sklearn.model_selection import train_test_split
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
from gammatone.gtgram import gtgram
from sklearn.metrics import confusion_matrix

# ==============================
# Audio Preprocessing Functions
# ==============================

def preprocess_audio(file_path, frame_length=2048, hop_length=512, show_example=False):
    """
    Applies pre-emphasis, framing, and windowing to the audio signal.

    Args:
        file_path (str): Path to the audio file.
        frame_length (int): Length of each frame.
        hop_length (int): Overlap between frames.
        show_example (bool): Whether to plot the audio and emphasized signal.

    Returns:
        tuple: Emphasized audio, framed and windowed signal, sample rate.
    """
    audio, sample_rate = librosa.load(file_path, sr=None)

    # Apply pre-emphasis
    pre_emphasis = 0.97
    emphasized_audio = np.append(audio[0], audio[1:] - pre_emphasis * audio[:-1])

    # Frame the signal
    frames = librosa.util.frame(emphasized_audio, frame_length=frame_length, hop_length=hop_length).T

    # Apply Hamming window
    windowed_frames = frames * np.hamming(frame_length)

    # Optional visualization
    if show_example:
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 2, 1)
        plt.title("Original Audio")
        librosa.display.waveshow(audio, sr=sample_rate)
        plt.subplot(1, 2, 2)
        plt.title("Emphasized Audio")
        librosa.display.waveshow(emphasized_audio, sr=sample_rate)
        plt.tight_layout()
        plt.show()

        plt.figure(figsize=(12, 4))
        plt.subplot(1, 2, 1)
        plt.title("Emphasized Audio")
        librosa.display.waveshow(emphasized_audio, sr=sample_rate)
        plt.subplot(1, 2, 2)
        plt.title("Emphasized Audio + Framing")
        librosa.display.waveshow(frames, sr=sample_rate)
        plt.tight_layout()
        plt.show()

        plt.figure(figsize=(12, 4))
        plt.subplot(1, 2, 1)
        plt.title("Emphasized Audio + Framing")
        librosa.display.waveshow(frames, sr=sample_rate)
        plt.subplot(1, 2, 2)
        plt.title("Emphasized Audio + Framing + Windowed")
        librosa.display.waveshow(windowed_frames, sr=sample_rate)
        plt.tight_layout()
        plt.show()

    return emphasized_audio, windowed_frames, sample_rate

# ============================
# Feature Extraction Functions
# ============================

def extract_features(file_path, frame_length=2048, hop_length=512, max_frames=100, n_features=13, save_path=None, show_example=False):
    """
    Extracts audio features including MFCC, GFCC, CQT, LOFAR, and their delta features.

    Args:
        file_path (str): Path to the audio file.
        frame_length (int): Length of each frame.
        hop_length (int): Overlap between frames.
        max_frames (int): Maximum number of frames per feature.
        n_features (int): Number of features to extract.
        save_path (str): Path to save the extracted features.
        show_example (bool): Whether to visualize the features.

    Returns:
        np.ndarray: Concatenated feature matrix.
    """

    def compute_gfcc(audio, sr, n_filters=64, frame_length=0.025, hop_length=0.01, f_min=50):
        # Aplicar filtro Gammatone
        gammatone_features = gtgram(audio, sr, frame_length, hop_length, n_filters, f_min)

        # Aplicar log para simular compressão cepstral
        gfcc = np.log(np.abs(gammatone_features) + 1e-8)

        return gfcc

    if save_path and os.path.exists(save_path):
        return np.load(save_path)

    emphasized_audio, _, sample_rate = preprocess_audio(file_path, frame_length, hop_length)

    # Extract features
    mfcc = librosa.feature.mfcc(y=emphasized_audio, sr=sample_rate, n_mfcc=n_features)
    print("mfcc", mfcc.shape)
    gfcc = compute_gfcc(emphasized_audio, sample_rate)
    print("gfcc", gfcc.shape)
    cqt = np.abs(librosa.cqt(y=emphasized_audio, sr=sample_rate, n_bins=84))
    print("cqt", cqt.shape)
    lofar = librosa.amplitude_to_db(np.abs(librosa.stft(emphasized_audio, n_fft=frame_length))[:n_features, :], ref=np.max)
    print("lofar", lofar.shape)
    delta_mfcc = librosa.feature.delta(mfcc)
    print("delta_mfcc", delta_mfcc.shape)
    delta_gfcc = librosa.feature.delta(gfcc)
    print("delta_gfcc", delta_gfcc.shape)
    delta_cqt = librosa.feature.delta(cqt)
    print("delta_cqt", delta_cqt.shape)
    delta_lofar = librosa.feature.delta(lofar)
    print("delta_lofar", delta_lofar.shape)

    # Pad or truncate features
    def pad_or_truncate(feature, max_frames):
        if feature.shape[1] > max_frames:
            return feature[:, :max_frames]
        elif feature.shape[1] < max_frames:
            return np.pad(feature, ((0, 0), (0, max_frames - feature.shape[1])), mode='constant')
        return feature

    mfcc = pad_or_truncate(mfcc, max_frames)
   # mfcc.shape

    gfcc = pad_or_truncate(gfcc, max_frames)
    cqt = pad_or_truncate(cqt, max_frames)
    lofar = pad_or_truncate(lofar, max_frames)
    delta_mfcc = pad_or_truncate(delta_mfcc, max_frames)
    delta_gfcc = pad_or_truncate(delta_gfcc, max_frames)
    delta_cqt = pad_or_truncate(delta_cqt, max_frames)
    delta_lofar = pad_or_truncate(delta_lofar, max_frames)

    # Fuse features
    mgcl = np.concatenate([mfcc, gfcc, cqt, lofar], axis=0)
    print("mgcl", mgcl.shape)
    delta_mgcl = np.concatenate([delta_mfcc, delta_gfcc, delta_cqt, delta_lofar], axis=0)
    print("delta_mgcl", delta_mgcl.shape)
    fused_features = np.concatenate([mgcl, delta_mgcl], axis=0)
    print("fused_features", fused_features.shape)
    
    # Save features
    if save_path:
        np.save(save_path, fused_features)

    # Optional visualization
    if show_example:
        features = {
            "MFCC": mfcc, "GFCC": gfcc, "CQT": cqt, "LOFAR": lofar,
            "Delta-MFCC": delta_mfcc, "Delta-GFCC": delta_gfcc,
            "Delta-CQT": delta_cqt, "Delta-LOFAR": delta_lofar,
            "MGCL": mgcl, "Delta-MGCL": delta_mgcl
        }
        for name, feature in features.items():
            plt.figure(figsize=(10, 4))
            librosa.display.specshow(feature, sr=sample_rate, x_axis='time', cmap='viridis')
            plt.colorbar(format='%+2.0f dB')
            plt.title(name)
            plt.tight_layout()
            plt.show()

    return fused_features


from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import NeighborhoodComponentsAnalysis

def reduce_and_visualize_with_nca(features, labels, threshold=0.3, chunk_size=5000):
    """
    Filters features based on their importance weights using NCA and a threshold, with block processing.
    ... (code from previous response) ...
    """
    print("Filtering features based on threshold using NCA...")

    num_samples, num_features, num_frames = features.shape
    print(f"Input shape: {features.shape}")

    # Initialize variables for storing results
    filtered_chunks = []
    nca = None  # Will hold the last trained NCA model

    # Process the data in chunks
    for i in range(0, num_samples, chunk_size):
        print(f"Processing block {i // chunk_size + 1}...")
        start = i
        end = min(i + chunk_size, num_samples)
        
        # Extract the current chunk of data
        chunk_features = features[start:end]  # Shape: (chunk_size, num_features, num_frames)
        chunk_labels = labels[start:end]

        # Vérification de la diversité des labels
        unique_labels = np.unique(chunk_labels)
        print(f"Unique labels in block {i // chunk_size + 1}: {unique_labels}")
        if len(unique_labels) <= 1:
            print("Warning: Only one label in this block!")

        # Collapse the temporal dimension (frames) by averaging across frames
        mean_chunk_features = np.mean(chunk_features, axis=2)  # Shape: (chunk_size, num_features)

        # Normaliser les caractéristiques
        scaler = StandardScaler()
        mean_chunk_features = scaler.fit_transform(mean_chunk_features)


        # Apply NCA to learn feature weights for the current block
        nca = NeighborhoodComponentsAnalysis(random_state=42)
        
        try:
            nca.fit(mean_chunk_features, chunk_labels)
        except MemoryError:
            print("MemoryError: Reduce the dataset size or optimize memory usage.")
            return None, None

        print("nca.components_:", nca.components_)  # Ajout de cette ligne

        # Extract feature weights (coefficients of the linear transformation)
        feature_weights = np.linalg.norm(nca.components_, axis=0)  # Shape: (num_features,)

        # Screen features based on the threshold
        important_features_mask = feature_weights >= threshold
        filtered_chunk = chunk_features[:, important_features_mask, :]  # Keep all frames

        print(f"Block {i // chunk_size + 1}:")
        print(f" - Number of features before filtering: {num_features}")
        print(f" - Number of features after filtering: {filtered_chunk.shape[1]}")

        # Append filtered chunk to results
        filtered_chunks.append(filtered_chunk)

    # Combine all filtered chunks into a single array
    final_filtered_features = np.vstack(filtered_chunks)

    # Visualize feature weights (optional, based on the last block)
    plt.figure(figsize=(10, 6))
    plt.plot(range(len(feature_weights)), feature_weights, 'ro')  # Changed to scatter plot with red circles
    plt.axhline(y=threshold, color='red', linestyle='--', label=f'Threshold = {threshold}')
    plt.title("Feature Weights (NCA)")
    plt.xlabel("Feature Index")
    plt.ylabel("Weight")
    plt.legend()
    plt.tight_layout()
    plt.show()

    return final_filtered_features, nca




# ======================
# Model Construction
# ======================

def build_resnet18(input_shape, num_classes):
    """
    Builds a ResNet18 model for classification.

    Args:
        input_shape (tuple): Shape of the input features.
        num_classes (int): Number of output classes.

    Returns:
        keras.Model: Compiled ResNet18 model.
    """
    inputs = layers.Input(shape=input_shape)
    x = layers.Conv2D(64, kernel_size=7, strides=2, padding='same')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)

    def resnet_block(x, filters, downsample=False):
        shortcut = x
        if downsample:
            shortcut = layers.Conv2D(filters, kernel_size=1, strides=2, padding='same')(shortcut)
        x = layers.Conv2D(filters, kernel_size=3, strides=(2 if downsample else 1), padding='same')(x)
        x = layers.BatchNormalization()(x)
        x = layers.ReLU()(x)
        x = layers.Conv2D(filters, kernel_size=3, padding='same')(x)
        x = layers.BatchNormalization()(x)
        x = layers.add([x, shortcut])
        x = layers.ReLU()(x)
        return x

    for filters, num_blocks in zip([64, 128, 256, 512], [2, 2, 2, 2]):
        for i in range(num_blocks):
            x = resnet_block(x, filters, downsample=(i == 0 and filters != 64))

    x = layers.GlobalAveragePooling2D()(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = models.Model(inputs, outputs)
    model.compile(optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Function to process a single audio file
def process_audio_file(args):
    """
    Processes a single audio file to extract features.

    Args:
        args (tuple): Contains the file path, class index, frame length, hop length, 
                      maximum frames, number of features, and save directory.

    Returns:
        tuple: A tuple containing the extracted features and class index.
    """
    file_path, class_index, frame_length, hop_length, max_frames, n_features, save_dir = args
    save_path = None
    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
        save_path = os.path.join(save_dir, os.path.basename(file_path) + '.npy')
    
    # Extract features from the audio file
    features = extract_features(file_path, frame_length, hop_length, max_frames, n_features, save_path=save_path)
    return features, class_index

# Function to load a dataset organized in subdirectories using ThreadPoolExecutor
def load_dataset(dataset_path, frame_length=2048, hop_length=512, max_frames=100, n_features=13, save_dir=None):
    """
    Loads a dataset organized in subdirectories, processes the audio files in parallel, 
    and extracts their features.

    Args:
        dataset_path (str): Path to the root directory containing the dataset.
        frame_length (int): Length of each frame for feature extraction.
        hop_length (int): Hop length for feature extraction.
        max_frames (int): Maximum number of frames per feature vector.
        n_features (int): Number of features to extract.
        save_dir (str, optional): Directory to save the extracted features.

    Returns:
        tuple: A tuple containing the features matrix (X), the labels (y), and 
               a dictionary mapping class names to indices.
    """
    X, y = [], []
    classes = sorted(os.listdir(dataset_path))  # List all class folders
    class_to_index = {cls_name: idx for idx, cls_name in enumerate(classes)}  # Map class names to indices

    tasks = []
    for cls_name in classes:
        cls_path = os.path.join(dataset_path, cls_name)
        if os.path.isdir(cls_path):  # Check if it is a directory
            for file_name in os.listdir(cls_path):  # List all files in the class folder
                file_path = os.path.join(cls_path, file_name)
                if file_name.endswith(('.wav', '.mp3')):  # Check if the file is an audio file
                    tasks.append((file_path, class_to_index[cls_name], frame_length, hop_length, max_frames, n_features, save_dir))

    # Process the files in parallel using ThreadPoolExecutor
    with ThreadPoolExecutor() as executor:
        results = list(tqdm(executor.map(process_audio_file, tasks), total=len(tasks), desc="Processing audio files"))
        for features, label in results:
            X.append(features)
            y.append(label)

    return np.array(X), np.array(y), class_to_index



# ======================
# Main Workflow
# ======================

def main():
    version = "V_5000"
    dataset_path_train = fr'/tools/mohand_postdoc/datasets/DeepShip/ClassesrandomlySplitted/train_DeepShip_Segments_5000/'
    dataset_path_test = fr'/tools/mohand_postdoc/datasets/DeepShip/ClassesrandomlySplitted/test_DeepShip_Segments_3500/'
    save_dir_train = fr'/tools/mohand_postdoc/datasets/DeepShip/ClassesrandomlySplitted/trainDeepShip_{version}_processed_features'
    save_dir_test = fr'/tools/mohand_postdoc/datasets/DeepShip/ClassesrandomlySplitted/testDeepShip_{version}_processed_features'
    model_save_path = fr"/tools/mohand_postdoc/datasets/models/resnet18_model_{version}_.h5"

    # Charger les datasets train et test
    print("Chargement des données d'entraînement...")
    X_train_raw, y_train, class_to_index = load_dataset(dataset_path_train, save_dir=save_dir_train)
    
    print("Chargement des données de test...")
    X_test_raw, y_test, _ = load_dataset(dataset_path_test, save_dir=save_dir_test)

    # Réduction de dimensionnalité et visualisation (appliquée uniquement sur train)
    print("Réduction de dimensionnalité sur les données d'entraînement...")
    X_train_reduced, nca_model = reduce_and_visualize_with_nca(X_train_raw, y_train)
    
    # Appliquer la transformation NCA sur les données de test
    num_samples_test = X_test_raw.shape[0]
    num_features_test = np.prod(X_test_raw.shape[1:])
    flattened_X_test = X_test_raw.reshape(num_samples_test, num_features_test)
    X_test_reduced = nca_model.transform(flattened_X_test)

    # Reshape des données pour le modèle
    X_train = X_train_reduced.reshape(X_train_reduced.shape[0], 2, 1, 1)
    X_test = X_test_reduced.reshape(X_test_reduced.shape[0], 2, 1, 1)

    # Construire ou charger le modèle
    if os.path.exists(model_save_path):
        print("Chargement du modèle sauvegardé...")
        model = models.load_model(model_save_path)
    else:
        print("Entraînement du modèle...")
        model = build_resnet18(input_shape=X_train.shape[1:], num_classes=len(class_to_index))
        early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)
        
        # Validation croisée sur les données d'entraînement
        history = model.fit(
            X_train,
            y_train,
            epochs=100,
            batch_size=5,
            validation_data=(X_test, y_test),
            callbacks=[early_stopping, lr_scheduler]
        )
        
        model.save(model_save_path)
        print(f"Modèle sauvegardé dans {model_save_path}")

    # Évaluation globale du modèle sur le jeu de test
    print("\nÉvaluation sur le jeu de test...")
    loss, accuracy = model.evaluate(X_test, y_test)
    print(f"Accuracy globale: {accuracy * 100:.2f}%")

    # 🔥 Ajout du calcul de l'accuracy par classe 🔥
    print("\n🔍 Calcul de l'accuracy par classe...")

    # Prédictions sur le jeu de test
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)  # Convertir les probabilités en indices de classes
    y_true_classes = y_test  # y_test contient déjà les indices des classes

    # Matrice de confusion
    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)

    # Compter le nombre d'échantillons par classe dans y_test
    import collections
    counts = collections.Counter(y_true_classes)
    print("\n📊 Nombre d'échantillons par classe dans y_test:")
    for class_id, count in sorted(counts.items()):
        print(f"Classe {class_id}: {count} échantillons")

    # Accuracy par classe
    accuracy_per_class = conf_matrix.diagonal() / conf_matrix.sum(axis=1)

    # ✅ Vérifier les classes présentes dans y_test
    classes_presentes = np.unique(y_true_classes)  # Liste des classes présentes dans y_test
    print(f"\n🔢 Classes présentes dans y_test: {classes_presentes.tolist()} ({len(classes_presentes)} classes)\n")

    # Affichage des résultats par classe
    print("\n📊 Résultats par classe:")
    for class_name, idx in class_to_index.items():
        if idx in classes_presentes:  # Vérifie si la classe est bien présente
            print(f"Classe {class_name} (index {idx}) - Accuracy: {accuracy_per_class[idx] * 100:.2f}%")
        else:
            print(f"⚠️ Classe {class_name} (index {idx}) n'a pas d'échantillons dans y_test.")

if __name__ == "__main__":
    main()

